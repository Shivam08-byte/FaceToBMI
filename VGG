{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "from random import randint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cuda\n"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([50000, 3, 32, 32])\ntorch.Size([10000, 3, 32, 32])\n"
    }
   ],
   "source": [
    "from utils import check_cifar_dataset_exists\n",
    "data_path = check_cifar_dataset_exists()\n",
    "train_data = torch.load(data_path+\"/cifar/train_data.pt\")\n",
    "train_label = torch.load(data_path+\"/cifar/train_label.pt\")\n",
    "test_data = torch.load(data_path+\"/cifar/test_data.pt\")\n",
    "test_label = torch.load(data_path+\"/cifar/test_label.pt\")\n",
    "# Review data\n",
    "print(train_data.size())\n",
    "print(test_data.size())\n",
    "mean = train_data.mean().to(device)\n",
    "std = train_data.std().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a VGG net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "\n",
    "        #Block 1: 3x32x32 -> 64x16x16\n",
    "        self.conv1a = nn.Conv2d(3,64, kernel_size=3, padding=1)\n",
    "        self.conv1b = nn.Conv2d(64,64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        #Block 2: 64x16x16 -> 128x8x8\n",
    "        self.conv2a = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2b = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        #Block 3: 128x8x8 -> 256x4x4\n",
    "        self.conv3a = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3b = nn.Conv2d(256,256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        #Block 4: 256x4x4 -> 512 x 2 x 2\n",
    "        self.conv4a = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        #Block 5: 512x2x2 -> 2048 -> 4096 -> 4096 -> 10\n",
    "        self.linear1 = nn.Linear(512*2*2, 4096, bias=False)\n",
    "        self.linear2 = nn.Linear(4096, 4096, bias=False)\n",
    "        self.linear3 = nn.Linear(4096, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #block 1 3x32x32 -> 64x16x16\n",
    "        x = self.conv1a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        #block 2 64x16x16 -> 128x8x8\n",
    "        x = self.conv2a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        #block 3 128x8x8 -> 256x4x4\n",
    "        x = self.conv3a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # block 4 256x4x4 -> 512x2x2\n",
    "        x = self.conv4a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # block 5 512x2x2 -> 2048 -> 4096 -> 4096 -> 10\n",
    "        x = x.view(-1, 2048)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose criterion, batch size, and initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "bs = 128\n",
    "my_lr = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "    running_error = 0\n",
    "    num_batches = 0\n",
    "    for count in range(0, 10000, bs):\n",
    "        minibatch_data = test_data[count:count+bs].to(device)\n",
    "        minibatch_label = test_label[count:count+bs].to(device)\n",
    "\n",
    "        inputs = (minibatch_data-mean)/std\n",
    "        \n",
    "        scores = net(inputs)\n",
    "\n",
    "        error = utils.get_error(scores, minibatch_label)\n",
    "        running_error += error.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    total_error = running_error/num_batches*100\n",
    "    print(\"Error on test set={:.2f}%\".format(total_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running 20 epochs on the trainning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "VGG(\n  (conv1a): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2a): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3a): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv4a): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (linear1): Linear(in_features=2048, out_features=4096, bias=False)\n  (linear2): Linear(in_features=4096, out_features=4096, bias=False)\n  (linear3): Linear(in_features=4096, out_features=10, bias=False)\n)\nThere are 27532352 (27.53 million) parameters in this neural network\n"
    }
   ],
   "source": [
    "net = VGG().to(device)\n",
    "print(net)\n",
    "utils.display_num_param(net)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(20):\n",
    "    running_error = 0\n",
    "    running_loss = 0\n",
    "    num_batches = 0\n",
    "    if epoch in [10, 14, 18]:\n",
    "        my_lr = my_lr/2\n",
    "    optimizer = optim.SGD(net.parameters(), lr=my_lr)\n",
    "    shuffled_indices = torch.randperm(50000)\n",
    "    for count in range(0, 50000, bs):\n",
    "        # Reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "        minibatch_data = train_data[count:count+bs].to(device)\n",
    "        minibatch_label = train_label[count:count+bs].to(device)\n",
    "\n",
    "        # normalize the minibatch (this is the only difference compared to before!)\n",
    "        inputs = (minibatch_data - mean)/std\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        scores = net(inputs)\n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate state\n",
    "        running_loss += loss.detach().item()\n",
    "        error = utils.get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Compute stats on epoch\n",
    "    total_error = running_error/num_batches*100\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (start - time.time())/60\n",
    "\n",
    "    print(\"Epoch: {}, Time: {:.2f} min, Learn Rate: {:.2f}, Loss:{:.4f}. Error:{:.4f}%\".format(epoch, elapsed, my_lr, total_loss, total_error))\n",
    "    eval_on_test_set()\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running a sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = randint(0, 10000-1)\n",
    "image = test_data[idx]\n",
    "utils.show(image)\n",
    "image = (image.to(device)-mean)/std\n",
    "scores = net(image.view(1,3,32,32))\n",
    "probs = F.softmax(scores, dim=1)\n",
    "utils.show_prob_cifar(probs.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}