{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitf2bconda1887e22467f64f0497eae62b7c5e5f5e",
   "display_name": "Python 3.7.6 64-bit ('f2b': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from glob import glob\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import OrderedDict\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Annotation data:  1026\n"
    }
   ],
   "source": [
    "annotation_df = pd.read_csv(data_path+'annotation.csv')\n",
    "annotation_df.head()\n",
    "print(\"Annotation data: \",len(annotation_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 1026 photos \n"
    }
   ],
   "source": [
    "all_files = glob(data_path+\"images/*\")\n",
    "all_jpgs = sorted([img for img in all_files if \".jpg\" in img or \".jpeg\" in img or \"JPG\" in img])\n",
    "print(\"Total {} photos \".format(len(all_jpgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_path = [image for image in all_jpgs ]\n",
    "image_df = pd.DataFrame(id_path, columns=['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                           path  image  height  weight        BMI\n0  ../data/raw/images/f_001.jpg  f_001    1.55    61.0  25.390219\n1  ../data/raw/images/f_002.jpg  f_002    1.76    85.0  27.440599\n2  ../data/raw/images/f_003.jpg  f_003    1.78    56.0  17.674536\n3  ../data/raw/images/f_004.jpg  f_004    1.63    63.0  23.711845\n4  ../data/raw/images/f_005.jpg  f_005    1.76    54.0  17.432851",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>image</th>\n      <th>height</th>\n      <th>weight</th>\n      <th>BMI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../data/raw/images/f_001.jpg</td>\n      <td>f_001</td>\n      <td>1.55</td>\n      <td>61.0</td>\n      <td>25.390219</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../data/raw/images/f_002.jpg</td>\n      <td>f_002</td>\n      <td>1.76</td>\n      <td>85.0</td>\n      <td>27.440599</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../data/raw/images/f_003.jpg</td>\n      <td>f_003</td>\n      <td>1.78</td>\n      <td>56.0</td>\n      <td>17.674536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../data/raw/images/f_004.jpg</td>\n      <td>f_004</td>\n      <td>1.63</td>\n      <td>63.0</td>\n      <td>23.711845</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../data/raw/images/f_005.jpg</td>\n      <td>f_005</td>\n      <td>1.76</td>\n      <td>54.0</td>\n      <td>17.432851</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "full_df = image_dataframe.merge(profile_dataframe, left_index=True, right_index=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Image path: ../data/raw/images/f_066.jpg\nImage name: f_066\nValues shape: (1, 3)\nheight: 1.77 m, weight: 55.0 kg, bmi: 17.55561939\n"
    }
   ],
   "source": [
    "n = 65\n",
    "image_path = full_df.iloc[n, 0]\n",
    "image_name = full_df.iloc[n, 1]\n",
    "values = np.asarray(full_df.iloc[n, 2:])\n",
    "values = values.astype('float').reshape(-1, 3)\n",
    "\n",
    "print('Image path: {}'.format(image_path))\n",
    "print('Image name: {}'.format(image_name))\n",
    "print('Values shape: {}'.format(values.shape))\n",
    "print('height: {} m, weight: {} kg, bmi: {}'.format(*values[:, 0], *values[:, 1], *values[:, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class FaceToBMIDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.annotaion = pd.read_csv(csv_file)\n",
    "        self.image_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotaion)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir, self.annotaion.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        values = self.values_frame.iloc[idx, 1:]\n",
    "        values = np.array([values])\n",
    "        values = values.astype('float').reshape(-1, 3)\n",
    "        item = {'image': image, 'values': values}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  }
 ]
}