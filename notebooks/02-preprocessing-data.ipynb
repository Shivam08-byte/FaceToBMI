{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitf2bconda1887e22467f64f0497eae62b7c5e5f5e",
   "display_name": "Python 3.7.6 64-bit ('f2b': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from glob import glob\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import OrderedDict\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Annotation data:  1026\n"
    }
   ],
   "source": [
    "annotation_df = pd.read_csv(data_path+'annotation.csv')\n",
    "annotation_df.head()\n",
    "print(\"Annotation data: \",len(annotation_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 1026 photos \n"
    }
   ],
   "source": [
    "all_files = glob(data_path+\"images/*\")\n",
    "all_jpgs = sorted([img for img in all_files if \".jpg\" in img or \".jpeg\" in img or \"JPG\" in img])\n",
    "print(\"Total {} photos \".format(len(all_jpgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_path = [image for image in all_jpgs ]\n",
    "image_df = pd.DataFrame(id_path, columns=['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                           path  height  weight        BMI\n0  ../data/raw/images/f_001.jpg    1.55    61.0  25.390219\n1  ../data/raw/images/f_002.jpg    1.76    85.0  27.440599\n2  ../data/raw/images/f_003.jpg    1.78    56.0  17.674536\n3  ../data/raw/images/f_004.jpg    1.63    63.0  23.711845\n4  ../data/raw/images/f_005.jpg    1.76    54.0  17.432851",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>height</th>\n      <th>weight</th>\n      <th>BMI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../data/raw/images/f_001.jpg</td>\n      <td>1.55</td>\n      <td>61.0</td>\n      <td>25.390219</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../data/raw/images/f_002.jpg</td>\n      <td>1.76</td>\n      <td>85.0</td>\n      <td>27.440599</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../data/raw/images/f_003.jpg</td>\n      <td>1.78</td>\n      <td>56.0</td>\n      <td>17.674536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../data/raw/images/f_004.jpg</td>\n      <td>1.63</td>\n      <td>63.0</td>\n      <td>23.711845</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../data/raw/images/f_005.jpg</td>\n      <td>1.76</td>\n      <td>54.0</td>\n      <td>17.432851</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "full_df = image_df.merge(annotation_df, left_index=True, right_index=True)\n",
    "full_df = full_df.drop(['image'], axis=1)\n",
    "full_df.to_csv(\"../data/interim/full_annotation.csv\", index=False, header=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Image path: ../data/raw/images/f_066.jpg\nValues shape: (1, 3)\nheight: 1.77 m, weight: 55.0 kg, bmi: 17.55561939\n"
    }
   ],
   "source": [
    "n = 65\n",
    "image_path = full_df.iloc[n, 0]\n",
    "values = np.asarray(full_df.iloc[n, 1:])\n",
    "values = values.astype('float').reshape(-1, 3)\n",
    "\n",
    "print('Image path: {}'.format(image_path))\n",
    "print('Values shape: {}'.format(values.shape))\n",
    "print('height: {} m, weight: {} kg, bmi: {}'.format(*values[:, 0], *values[:, 1], *values[:, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class FaceToBMIDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transform=transforms.ToTensor()):\n",
    "        self.annotaion = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotaion)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx): idx = idx.tolist()\n",
    "        img_path = self.annotaion.iloc[idx,0]\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        values = self.annotaion.iloc[idx, 1:]\n",
    "        values = np.asarray(values)\n",
    "        values = values.astype('float').reshape(-1, 3)\n",
    "        # item = {\"image\": image, \"values\": torch.from_numpy(values)}\n",
    "        # return item\n",
    "        return image, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = \"../data/interim/full_annotation.csv\"\n",
    "image_dir = \"../data/raw/images\"\n",
    "train_data = FaceToBMIDataset(csv_file=path_to_csv, image_dir=image_dir, transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'torch.Tensor'>\n<class 'torch.Tensor'>\n"
    }
   ],
   "source": [
    "images, values = next(iter(train_loader))\n",
    "print(type(images))\n",
    "print(type(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([16, 3, 224, 224])\ntorch.Size([16, 1, 3])\n"
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[  1.8200, 102.0000,  30.7934]],\n\n        [[  2.2100, 119.0000,  24.3648]],\n\n        [[  1.8500,  98.0000,  28.6340]],\n\n        [[  1.8000,  73.0000,  22.5309]],\n\n        [[  1.8300,  72.0000,  21.4996]],\n\n        [[  1.6500,  51.0000,  18.7328]],\n\n        [[  1.6800,  58.0000,  20.5499]],\n\n        [[  1.7000,  62.0000,  21.4533]],\n\n        [[  1.8500,  82.0000,  23.9591]],\n\n        [[  1.6300,  58.0000,  21.8300]],\n\n        [[  1.9300,  82.0000,  22.0140]],\n\n        [[  1.7500,  71.0000,  23.1837]],\n\n        [[  1.7800,  78.0000,  24.6181]],\n\n        [[  1.8000,  64.0000,  19.7531]],\n\n        [[  2.1100, 110.0000,  24.7074]],\n\n        [[  2.0300, 113.0000,  27.4212]]], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}